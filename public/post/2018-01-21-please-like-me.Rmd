---
title: Please like me
author: ~
date: '2018-01-21'
slug: please-like-me
categories:
  - R
tags:
  - rvest
  - wordcloud
  - tidytext
---
```{r packages, include=FALSE}

library(wordcloud)
library(tidyr)
library(reshape2)
library(rvest)
library(tidytext)
library(tidyverse)
```

# Sundays

When I woke up this morning I wrote a long to do list. Instead of working through the list I somehow ended up spending most of my day playing in R. 

# Scraping the web
I have been keen to try web scraping in R for a while so I gave `rvest` a go. I was a bit too restless to actually read any tutorials so I just started trying different commands. To be honest I think that was a mistake as I spent quite a while without anything to show for it.

The title of this blog post is not a desperate cry for attention. The Australian show "Please Like Me" is probably my favourite show ever. It made me laugh and it made me cry and one of the main characters is a super cute dog. I wanted to scrape data from IMBD and I wanted to start out simple so I tried getting a list of the characters. I already had selectorgadget (http://selectorgadget.com/) installed which I used to identify parts of the website. 

```{r IMBD}
PLM_IMDB <- read_html("http://www.imdb.com/title/tt2155025")
character <- html_nodes(PLM_IMDB, ".character")
html_text(character)

```
As you can see this is quite messy and I only have 9 names even though I can see way more on the website. I tried a few different options and google and suggested I should look for the word "table". I was sick of IMDB and turned to wikipedia. As you can see below the data frame that I  get is not very tidy. 
```{r IMBD table}
website <- read_html("https://en.wikipedia.org/wiki/List_of_Please_Like_Me_episodes") 
cast <- html_nodes(website, xpath = '//*[@id="mw-content-text"]/div/table[2]')
table1 <- html_table(cast) %>% 
  as.data.frame()
table2 <- html_table(cast, fill=TRUE) %>% 
  as.data.frame()
table3 <- html_table(cast, header=TRUE) %>% 
  as.data.frame()

head(table1$Title,3)
```
Again, I tried a few different ways but I just couldn't create a nice table. I was becoming more and more disillusioned so I decided to try to extract one column at a time and then combine them. 

```{r}
website <- read_html("https://en.wikipedia.org/wiki/List_of_Please_Like_Me_episodes") 
PLM_nodes  <- website %>% 
  html_nodes(".summary")

PLM_nodes1  <- website %>% 
  html_nodes(".description")

PLM_text <-html_text(PLM_nodes)
PLM_text1 <-html_text(PLM_nodes1)

PLM <- data_frame(PLM_text, PLM_text1)
names(PLM) <- c("episode_title", "description")

```

It worked! When I had been thinking about web scraping and I always had some kind of sentiment analysis in mind. By the time I actually had some data it was pretty late so I have only scraped the surface of fun things I can do with this data but here is my first attempt. 

# Getting that sweet word cloud
Let's split the description variable so that each word is on a separate row and then we can look at the most common words. 
```{r}
tidy_PLM <- PLM %>%
  unnest_tokens(output=word, description, to_lower = TRUE)

cleaned_PLM <- tidy_PLM %>% 
  anti_join(stop_words, by="word") # remove stop words (words that are very common)

cleaned_PLM %>%
  count(word, sort = TRUE) %>% 
  head(10)
```
Almost all of the top 10 words are character names.

One of my favourite characters is John the dog and I wanted to see how often he was mentioned and in what context. I split the data frame into sentences instead of words.

```{r John the dog}
tidy_PLM_sentence <- PLM %>%
  unnest_tokens(sentence, description, token = "sentences", to_lower = FALSE) %>% 
  mutate(john=str_detect(sentence, regex("john", ignore_case = TRUE)))

tidy_PLM_sentence %>% 
  filter(john==TRUE) %>% 
  select(sentence)
```
I did of course create the obligatory word cloud. It probably would have been a good idea to remove the character names. 

```{r}
cleaned_PLM %>%
  count(word) %>% 
  with(wordcloud(word, n, max.words = 100))

```

# Next steps

The coded I ended up with is very different to that I set out to write. In hindsight I should have read a few `rvest` and `tidytext` tutorials as this would have saved me time. There are so many things I want to with this data. I want to add the season and episode number to the data set to see if the language changed over time. I should probably remove the character names from the data I use for the word cloud so that is shows the topics of show rather than just the names of the characters. Mostly I want to include a picture of John the dog.  Since this blog is as much about the learning process as the actual results of any code I decided to publish the few successful lines of code that I wrote today.


# Packages I used
```{r, eval=FALSE}

library(wordcloud)
library(tidyr)
library(reshape2)
library(rvest)
library(tidytext)
library(tidyverse)
```

